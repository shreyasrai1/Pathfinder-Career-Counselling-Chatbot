{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOeVZcQ8tiKLI1cFZUa3/4k"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_yNFD5nNTKo"
      },
      "outputs": [],
      "source": [
        "# Install required libraries silently\n",
        "!pip install -q \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-huggingface \\\n",
        "    transformers \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    requests==2.32.4 \\\n",
        "    huggingface-hub \\\n",
        "    google-search-results # <--- THE FIX IS FORCING THE VERSION ABOVE\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "\n",
        "# Transformers Imports (for 4-bit loading)\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Secure API Key Setup\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"Attempting to set API keys...\")\n",
        "\n",
        "# --- Hugging Face Login & Token ---\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token, add_to_git_credential=False)\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "    print(\"‚úÖ Successfully logged in to Hugging Face and set HF_TOKEN env variable.\")\n",
        "else:\n",
        "    print(\"‚ùå HF_TOKEN secret not found. Model loading in Cell 3 will fail.\")\n",
        "\n",
        "# --- Serper (Web Search) API Key ---\n",
        "serper_api_key = userdata.get('SERPER_API_KEY')\n",
        "if serper_api_key:\n",
        "    os.environ['SERPER_API_KEY'] = serper_api_key\n",
        "    print(\"‚úÖ Successfully set SERPER_API_KEY env variable.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: SERPER_API_KEY secret not found. Web search will be skipped.\")\n",
        "    print(\"Please set 'SERPER_API_KEY' in Colab Secrets for web-augmented results.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Dependencies Installed and Keys Configured ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 2. Data, Embeddings, and Vector Store Setup\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Small sample of role descriptions\n",
        "data = {\n",
        "    \"role\": [\"Data Analyst\", \"Software Engineer\", \"Business Analyst\", \"AI Engineer\", \"UI/UX Designer\"],\n",
        "    \"description\": [\n",
        "        \"Analyze datasets to derive insights, build dashboards, and support decision making.\",\n",
        "        \"Develop and maintain software systems, write efficient code, test and debug.\",\n",
        "        \"Bridge business needs and data; communicate insights to stakeholders.\",\n",
        "        \"Design and train machine learning models, optimize algorithms.\",\n",
        "        \"Design user interfaces and experiences that are intuitive and appealing.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Convert DataFrame to LangChain Document objects\n",
        "# This stores the 'role' as metadata for potential future use\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=row[\"description\"],\n",
        "        metadata={\"role\": row[\"role\"]}\n",
        "    ) for _, row in df.iterrows()\n",
        "]\n",
        "print(f\"Created {len(documents)} LangChain Documents.\")\n",
        "\n",
        "# 2. Load Embedding Model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
        ")\n",
        "\n",
        "# 3. Create FAISS Vector Store\n",
        "# This handles embedding the documents and indexing them in one step\n",
        "try:\n",
        "    vector_store = FAISS.from_documents(documents, embeddings)\n",
        "    print(\"\\nFAISS vector store created successfully.\")\n",
        "\n",
        "    # 4. Create Retriever\n",
        "    # This object is the \"search function\" for our RAG chain\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "    print(\"Retriever object created.\")\n",
        "\n",
        "    # 5. Initialize Web Search Tool\n",
        "    search_tool = GoogleSerperAPIWrapper()\n",
        "    # You can test it with: print(search_tool.run(\"what is langchain?\"))\n",
        "    print(\"Google Serper web search tool initialized.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error creating vector store: {e}\")\n",
        "    print(\"This might be a network issue or an problem with the embedding model.\")\n",
        "\n",
        "print(\"\\n--- Knowledge Base and Tools Ready ---\")"
      ],
      "metadata": {
        "id": "LO5bAHp-SOxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 3. LLM Setup (Mistral-7B-Instruct-v0.3)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# --- FIX: Moved the new import to the top of the cell ---\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "try:\n",
        "    # 4-bit Quantization configuration\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Load model using 4-bit quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Initialize the transformers pipeline\n",
        "    gen_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        return_full_text=False  # <--- THIS IS THE FIX\n",
        "    )\n",
        "\n",
        "    # --- LangChain Wrapper (FIXED INDENTATION) ---\n",
        "    # This logic is now correctly indented INSIDE the try block\n",
        "\n",
        "    # 1. Wrap the raw transformers pipeline in LangChain's HuggingFacePipeline\n",
        "    llm_pipeline = HuggingFacePipeline(pipeline=gen_pipeline)\n",
        "\n",
        "    # 2. Wrap the LangChain pipeline in ChatHuggingFace for chat message compatibility\n",
        "    chat_llm = ChatHuggingFace(llm=llm_pipeline)\n",
        "    # ---------------------------------\n",
        "\n",
        "    print(f\"\\n‚úÖ Model {model_name} loaded and wrapped for LangChain successfully (4-bit).\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR: Model loading failed. \")\n",
        "    print(f\"Error details: {e}\")\n",
        "    chat_llm = None"
      ],
      "metadata": {
        "id": "CYM6xn5sSaBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 4. LangChain RAG Chain (Replaces pathfinder_agent)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "if chat_llm:\n",
        "    # 1. Define helper functions\n",
        "    def format_docs(docs):\n",
        "        \"\"\"Converts a list of Document objects into a single string.\"\"\"\n",
        "        return \"\\n\".join(\n",
        "            f\"{doc.metadata.get('role', 'Source')}: {doc.page_content}\" for doc in docs\n",
        "        )\n",
        "\n",
        "    # 2. Define the context retrieval step\n",
        "    context_retriever = RunnableParallel(\n",
        "        related_roles=retriever | format_docs,\n",
        "        web_info=RunnableLambda(lambda x: search_tool.run(x + \" career path salary india\")),\n",
        "        profile_text=RunnablePassthrough() # Passes the original profile text through\n",
        "    )\n",
        "\n",
        "    # 3. Define the Prompt (--- THIS IS THE UPGRADED PROMPT ---)\n",
        "    # The template uses the keys from context_retriever ('related_roles', 'web_info', 'profile_text')\n",
        "    system_prompt = \"\"\"\n",
        "You are Pathfinder AI, an expert career counsellor.\n",
        "Your task is to analyze the user's profile, especially their **likes, dislikes, and constraints**, and provide a structured career plan.\n",
        "\n",
        "Use the context below, but **you must critically evaluate if the retrieved roles fit the user's constraints.**\n",
        "If a retrieved role (like \"AI Engineer\") is a **bad fit** due to the user's constraints (e.g., \"dislikes math\"), you **must recommend against it** or suggest an alternative, less-known role that fits better (like \"MLOps Engineer\" or \"AI Product Manager\").\n",
        "\n",
        "Relevant job info:\n",
        "{related_roles}\n",
        "\n",
        "Recent web info:\n",
        "{web_info}\n",
        "\n",
        "Follow the steps exactly:\n",
        "1. **Critically analyze the user's profile.**\n",
        "2. Identify 2-3 **genuinely suitable** career roles. **Prioritize the user's constraints (likes/dislikes) over their general interests.**\n",
        "3. For each role, explain *why* it is a good fit, specifically addressing their constraints.\n",
        "4. List 3 key missing skills for each role.\n",
        "5. Suggest a 6-month learning roadmap with *specific, appropriate* courses and projects that respect the user's constraints (e.g., recommend \"AI for Everyone\" not a math-heavy course).\n",
        "\n",
        "Format the final output clearly using headings and bullet points.\n",
        "\"\"\"\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"user\", \"My profile is: {profile_text}\")\n",
        "    ])\n",
        "\n",
        "    # 4. Build the Full RAG Chain\n",
        "    # This is the complete, runnable agent\n",
        "    rag_chain = (\n",
        "        context_retriever |   # 1. Get context\n",
        "        prompt_template   |   # 2. Format prompt\n",
        "        chat_llm          |   # 3. Call LLM\n",
        "        StrOutputParser()     # 4. Get string output\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ LangChain RAG chain (LCEL) built successfully with **upgraded prompt**.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå LLM not loaded. Cannot create RAG chain.\")\n",
        "    rag_chain = None"
      ],
      "metadata": {
        "id": "RbC4BXQ9WMC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "# 5. Execution\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "profile = \"\"\"I am a 3rd year B.Tech CSE student.\n",
        "I enjoy working with data, love research and visualization,\n",
        "but dislike hardcore coding. I prefer hybrid or remote jobs.\"\"\"\n",
        "\n",
        "print(f\"--- Processing Profile ---\")\n",
        "print(profile)\n",
        "\n",
        "if rag_chain:\n",
        "    # .invoke() runs the entire chain\n",
        "    result = rag_chain.invoke(profile)\n",
        "\n",
        "    print(\"\\n--- Pathfinder AI Result (via LangChain) ---\")\n",
        "    print(result)\n",
        "else:\n",
        "    print(\"\\n--- Skipping execution as chain failed to build ---\")"
      ],
      "metadata": {
        "id": "Yds7S3S7WP_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# -------------------------------------------------------------------\n",
        "# LangChain Version - RAG Agent Streamlit App\n",
        "# -------------------------------------------------------------------\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Hugging Face & Transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from huggingface_hub import login\n",
        "\n",
        "# LangChain Imports\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFacePipeline\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Resource Loading (@st.cache_resource)\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    \"\"\"Loads the 4-bit quantized LLM and wraps it for LangChain.\"\"\"\n",
        "\n",
        "    # Read token from OS environment (set by the launch script)\n",
        "    hf_token = os.environ.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token, add_to_git_credential=False)\n",
        "    else:\n",
        "        st.error(\"HF_TOKEN environment variable not set. Cannot load model.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            MODEL_NAME,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        gen_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            max_new_tokens=1024,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            return_full_text=False  # <--- THIS IS THE FIX\n",
        "        )\n",
        "\n",
        "        # --- Wrap for LangChain (FIXED) ---\n",
        "        llm_pipeline = HuggingFacePipeline(pipeline=gen_pipeline)\n",
        "        chat_llm = ChatHuggingFace(llm=llm_pipeline)\n",
        "        return chat_llm\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(\"Failed to load LLM. Error: {}\".format(e))\n",
        "        return None\n",
        "\n",
        "@st.cache_resource\n",
        "def load_embedding_model():\n",
        "    \"\"\"Loads the embedding model.\"\"\"\n",
        "    try:\n",
        "        return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
        "    except Exception as e:\n",
        "        st.error(\"Failed to load embedding model. Error: {}\".format(e))\n",
        "        return None\n",
        "\n",
        "@st.cache_resource\n",
        "def create_knowledge_base(_embed_model): # <-- FIX: Added underscore to ignore for hashing\n",
        "    \"\"\"Creates the DataFrame and the FAISS vector store.\"\"\"\n",
        "    data = {\n",
        "        \"role\": [\"Data Analyst\", \"Software Engineer\", \"Business Analyst\", \"AI Engineer\", \"UI/UX Designer\"],\n",
        "        \"description\": [\n",
        "            \"Analyze datasets to derive insights, build dashboards, and support decision making.\",\n",
        "            \"Develop and maintain software systems, write efficient code, test and debug.\",\n",
        "            \"Bridge business needs and data; communicate insights to stakeholders.\",\n",
        "            \"Design and train machine learning models, optimize algorithms.\",\n",
        "            \"Design user interfaces and experiences that are intuitive and appealing.\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    documents = [\n",
        "        Document(page_content=desc, metadata={\"role\": role})\n",
        "        for role, desc in zip(data[\"role\"], data[\"description\"])\n",
        "    ]\n",
        "\n",
        "    vector_store = FAISS.from_documents(documents, _embed_model)\n",
        "    return vector_store\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. RAG Chain Definition\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def build_rag_chain(chat_llm, retriever, search_tool):\n",
        "    \"\"\"Builds the executable LangChain RAG agent chain.\"\"\"\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\".join(\n",
        "            f\"{doc.metadata.get('role', 'Source')}: {doc.page_content}\" for doc in docs\n",
        "        )\n",
        "\n",
        "    context_retriever = RunnableParallel(\n",
        "        related_roles=retriever | format_docs,\n",
        "        web_info=RunnableLambda(lambda x: search_tool.run(x + \" career path salary india\")),\n",
        "        profile_text=RunnablePassthrough()\n",
        "    )\n",
        "\n",
        "    # --- THIS IS THE UPGRADED PROMPT ---\n",
        "    system_prompt = \"\"\"\n",
        "You are Pathfinder AI, an expert career counsellor.\n",
        "Your task is to analyze the user's profile, especially their **likes, dislikes, and constraints**, and provide a structured career plan.\n",
        "\n",
        "Use the context below, but **you must critically evaluate if the retrieved roles fit the user's constraints.**\n",
        "If a retrieved role (like \"AI Engineer\") is a **bad fit** due to the user's constraints (e.g., \"dislikes math\"), you **must recommend against it** or suggest an alternative, less-known role that fits better (like \"MLOps Engineer\" or \"AI Product Manager\").\n",
        "\n",
        "Relevant job info:\n",
        "{related_roles}\n",
        "\n",
        "Recent web info:\n",
        "{web_info}\n",
        "\n",
        "Follow the steps exactly:\n",
        "1. **Critically analyze the user's profile.**\n",
        "2. Identify 2-3 **genuinely suitable** career roles. **Prioritize the user's constraints (likes/dislikes) over their general interests.**\n",
        "3. For each role, explain *why* it is a good fit, specifically addressing their constraints.\n",
        "4. List 3 key missing skills for each role.\n",
        "5. Suggest a 6-month learning roadmap with *specific, appropriate* courses and projects that respect the user's constraints (e.g., recommend \"AI for Everyone\" not a math-heavy course).\n",
        "\n",
        "Format the final output clearly using headings and bullet points.\n",
        "\"\"\"\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt),\n",
        "        (\"user\", \"My profile is: {profile_text}\")\n",
        "    ])\n",
        "\n",
        "    rag_chain = (\n",
        "        context_retriever |\n",
        "        prompt_template |\n",
        "        chat_llm |\n",
        "        StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Streamlit UI\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Pathfinder AI Career Counselor\", layout=\"wide\")\n",
        "    st.title(\"ü§ñ Pathfinder AI Career Counselor\")\n",
        "    st.caption(\"LangChain RAG Agent powered by Mistral-7B-Instruct-v0.3\")\n",
        "\n",
        "    # --- Load all resources ---\n",
        "    chat_llm = load_llm()\n",
        "    embed_model = load_embedding_model()\n",
        "\n",
        "    if not chat_llm or not embed_model:\n",
        "        st.error(\"Core models failed to load. The application cannot start.\")\n",
        "        st.stop()\n",
        "\n",
        "    # Check for Serper key\n",
        "    if not os.environ.get('SERPER_API_KEY'):\n",
        "        st.warning(\"SERPER_API_KEY not set. Web search results will be disabled.\", icon=\"‚ö†Ô∏è\")\n",
        "        search_tool = RunnableLambda(lambda x: \"Web search disabled. SERPER_API_KEY not provided.\")\n",
        "    else:\n",
        "        search_tool = GoogleSerperAPIWrapper()\n",
        "\n",
        "    vector_store = create_knowledge_base(_embed_model=embed_model)\n",
        "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # --- Build the single, reusable RAG chain ---\n",
        "    try:\n",
        "        rag_chain = build_rag_chain(chat_llm, retriever, search_tool)\n",
        "    except Exception as e:\n",
        "        st.error(\"Failed to build RAG chain: {}\".format(e))\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "    # --- Chat Interface ---\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    if profile_query := st.chat_input(\"Paste your career profile here to get an analysis...\"):\n",
        "        st.session_state.messages.append({\"role\": \"user\", \"content\": profile_query})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(profile_query)\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            with st.spinner(\"Analyzing profile and generating career roadmap... This may take up to 30 seconds.\"):\n",
        "\n",
        "                # --- Run the chain ---\n",
        "                response = rag_chain.invoke(profile_query)\n",
        "\n",
        "                st.markdown(response)\n",
        "\n",
        "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XUtZx-trWsHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ngrok and Streamlit libraries\n",
        "!pip install -q streamlit pyngrok \\\n",
        "    langchain \\\n",
        "    langchain-community \\\n",
        "    langchain-huggingface \\\n",
        "    transformers \\\n",
        "    sentence-transformers \\\n",
        "    faiss-cpu \\\n",
        "    accelerate \\\n",
        "    bitsandbytes \\\n",
        "    requests==2.32.4 \\\n",
        "    huggingface-hub \\\n",
        "    google-search-results\n",
        "\n",
        "# Authenticate Ngrok and set environment variables\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- 1. SET ENVIRONMENT VARIABLES ---\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "if HF_TOKEN:\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(\"‚úÖ HF_TOKEN loaded into environment variables.\")\n",
        "else:\n",
        "    print(\"‚ùå HF_TOKEN secret not found. Check Colab Secrets.\")\n",
        "\n",
        "# NEW: Set Serper API Key\n",
        "SERPER_API_KEY = userdata.get('SERPER_API_KEY')\n",
        "if SERPER_API_KEY:\n",
        "    os.environ['SERPER_API_KEY'] = SERPER_API_KEY\n",
        "    print(\"‚úÖ SERPER_API_KEY loaded into environment variables.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: SERPER_API_KEY secret not found. Web search in the app will be disabled.\")\n",
        "\n",
        "\n",
        "# --- 2. AUTHENTICATE NGROK ---\n",
        "NGROK_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
        "if NGROK_TOKEN:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"‚úÖ Ngrok Authenticated.\")\n",
        "else:\n",
        "    print(\"‚ùå NGROK_AUTH_TOKEN secret not found. Cannot start tunnel. Check Colab Secrets.\")\n",
        "\n",
        "# Disconnect any previous tunnels\n",
        "try:\n",
        "    ngrok.disconnect(ngrok.get_public_url())\n",
        "    print(\"‚úÖ Successfully disconnected previous Ngrok tunnel (if any).\")\n",
        "except:\n",
        "    print(\"No active tunnels found, proceeding to launch.\")\n",
        "\n",
        "# --- 3. START STREAMLIT AND NGROK ---\n",
        "print(\"Starting Streamlit server...\")\n",
        "\n",
        "# Kill any previous Streamlit process\n",
        "!killall streamlit >/dev/null 2>&1\n",
        "time.sleep(2)\n",
        "\n",
        "# Launch Streamlit. We pass BOTH tokens to the environment.\n",
        "!env HF_TOKEN=$HF_TOKEN SERPER_API_KEY=$SERPER_API_KEY nohup streamlit run app.py --server.port 8501 &\n",
        "\n",
        "# Give the server a moment to start\n",
        "time.sleep(10)\n",
        "\n",
        "# Get the Ngrok public URL\n",
        "try:\n",
        "    public_url = ngrok.connect(addr='8501', proto='http')\n",
        "    print(\"\\n--- Streamlit App is Running! üöÄ ---\")\n",
        "    print(\"Click the link below to access your Pathfinder AI Web App:\")\n",
        "    print(public_url)\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error connecting Ngrok tunnel. The Streamlit server may have failed to start. Error: {e}\")"
      ],
      "metadata": {
        "id": "1Oscp0TNWt9Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}